{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1636db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f98a5305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CIFAR10\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2.9%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "5.3%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "7.7%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "10.1%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "14.7%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "18.1%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "21.7%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "28.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "30.4%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "35.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "39.2%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "44.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "48.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "52.7%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "57.8%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "67.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "72.5%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "80.1%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "88.4%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "93.7%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "98.2%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CIFAR10\\cifar-10-python.tar.gz to CIFAR10\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=(-90,90)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    #transforms.RandomPerspective(distortion_scale=0.20, p=0.5), # random perspective transformation with a given probability.\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(\"CIFAR10\", train=True, transform=transform, download= True)\n",
    "testset = datasets.CIFAR10(\"CIFAR10\", train=False, transform=transform, download= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c25d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeeplearning_project\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "412f273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random', 'metrics': {'name': 'loss', 'goal': 'minimize'}, 'parameters': {'epochs': {'value': 25}, 'optimizer': {'values': ['adam', 'sgd', 'adamax']}, 'learning_rate': {'values': [0.001, 0.005, 0.01, 0.05]}, 'batch_size': {'values': [32, 64, 128]}, 'num_blocks': {'values': [(2, 2, 2)]}, 'in_planes': {'values': [43]}, 'k': {'values': [2]}}}\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random', \n",
    "    'metrics':{'name':'loss','goal':'minimize'}\n",
    "    }\n",
    "\n",
    "parameters_dict = {\n",
    "    'epochs':{'value':25},\n",
    "    'optimizer': {'values': ['adam', 'sgd','adamax']},  # 3 combinations\n",
    "    'learning_rate': {'values': [0.001, 0.005, 0.01, 0.05]}, # 4 combinations\n",
    "    'batch_size': {'values': [32, 64, 128]},                 # 4 combinations\n",
    "    'num_blocks': {'values': [(2,2,2)]}, # 3 combinations,(3,3,3),(4,4,4)\n",
    "    'in_planes':{'values': [43]},     # 7 combinations 21, 26, 29, 34, 38, \n",
    "    'k':{'values':[2]}, \n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdaf8010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. Additional properties are not allowed ('metrics' was unexpected)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ynexar88\n",
      "Sweep URL: https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/sweeps/ynexar88\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"ResNet-sweeps-local-sudu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fdaf4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep(config=None):\n",
    "    \n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # this config will be set by sweep controller, randomly assigned each time\n",
    "        config = wandb.config\n",
    "        \n",
    "        trainloader, testloader = loader(config.batch_size)\n",
    "        network = build_network(config.num_blocks, config.in_planes, config.k)\n",
    "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
    "        run_epochs = check_run(network, config.epochs, 1)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        history = train(network, run_epochs, optimizer, loss, trainloader, testloader,  device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c061b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_run(network, epochs, terminate_epochs):\n",
    "    total_params = sum(p.numel() for p in network.parameters())\n",
    "    if total_params<5000000:\n",
    "        run_epochs = epochs\n",
    "    else:\n",
    "        run_epochs = terminate_epochs\n",
    "    return run_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d4ffedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "    elif optimizer == \"adamax\":\n",
    "        optimizer = optim.Adamax(network.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    elif optimizer == \"rmsprop\":\n",
    "        optimizer = optim.RMSprop(network.parameters(), lr=learning_rate, alpha=0.99, eps=1e-08, weight_decay=0,\n",
    "                                  momentum=0, centered=False)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5c1885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_blocks-no of blocks in each layers(list,.ie(2,2,2)); in_planes--> first conv plane depth; k--> widening factor \n",
    "def build_network(num_blocks, in_planes, k):\n",
    "    if len(num_blocks) == 3:\n",
    "        network = ResNet3(BasicBlock, num_blocks, in_planes, k)\n",
    "    elif len(num_blocks) == 4:\n",
    "        network = ResNet4(BasicBlock, num_blocks, in_planes, k)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f1f057",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3460/2383872871.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mBasicBlock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_planes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplanes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBasicBlock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_planes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplanes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, kernel_size=3):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=kernel_size, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=kernel_size, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet4(nn.Module):\n",
    "    def __init__(self, block, num_blocks, in_planes, k=2, num_classes=10, kernel_size=3):\n",
    "        super(ResNet4, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.avg_pool_kernal_size = 4\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=kernel_size, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.layer1 = self._make_layer(block, k*self.in_planes, num_blocks[0], stride=1) # 32\n",
    "        self.layer2 = self._make_layer(block, k*self.in_planes, num_blocks[1], stride=2) # 64\n",
    "        self.layer3 = self._make_layer(block, k*self.in_planes, num_blocks[2], stride=2) #128\n",
    "        self.layer4 = self._make_layer(block, k*self.in_planes, num_blocks[3], stride=2) #256\n",
    "        self.linear = nn.Linear(self.in_planes, num_classes) #512 dense layers\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, self.kernel_size))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, self.avg_pool_kernal_size)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "class ResNet3(nn.Module):\n",
    "    def __init__(self, block, num_blocks, in_planes, k=2, num_classes=10, kernel_size=3):\n",
    "        super(ResNet3, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.avg_pool_kernal_size = 4\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=kernel_size, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.layer1 = self._make_layer(block, k*self.in_planes, num_blocks[0], stride=1) # 32\n",
    "        self.layer2 = self._make_layer(block, k*self.in_planes, num_blocks[1], stride=2) # 64\n",
    "        self.layer3 = self._make_layer(block, k*self.in_planes, num_blocks[2], stride=2) #128\n",
    "        #self.layer4 = self._make_layer(block, k*self.in_planes, num_blocks[3], stride=2) #256\n",
    "        self.linear = nn.Linear(4*self.in_planes, num_classes)  #512 dense layers\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, self.kernel_size))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        #out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, self.avg_pool_kernal_size)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb32da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, loss_fn, train_dl, val_dl, device='cuda'):\n",
    "    \n",
    "    #wandb.watch(model, loss_fn, log='all', log_freq=1000, log_graph=True)\n",
    "    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n",
    "          (type(model).__name__, type(optimizer).__name__,\n",
    "           optimizer.param_groups[0]['lr'], epochs, device))\n",
    "    \n",
    "    model.to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    wandb.log({'total_params':total_params})\n",
    "#     val_loss_min = np.Inf  # set valid loss to be infinity, will change when trainig loop starts\n",
    "#     history             = {}\n",
    "#     history['loss']     = []\n",
    "#     history['val_loss'] = []\n",
    "#     history['acc']      = []\n",
    "#     history['val_acc']  = []\n",
    "\n",
    "    start_time_sec = time.time()\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        # --- TRAIN AND EVALUATE ON TRAINING SET ----\n",
    "        model.train()\n",
    "        train_loss         = 0.0\n",
    "        num_train_correct  = 0\n",
    "        num_train_examples = 0\n",
    "\n",
    "        for batch in train_dl:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "            yhat = model(x)\n",
    "            loss = loss_fn(yhat,y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss          += loss.data.item() *x.size(0) # \n",
    "            num_train_correct   += (torch.max(yhat, 1)[1] == y).sum().item() # \n",
    "            num_train_examples  += x.shape[0]     \n",
    "\n",
    "        train_acc = num_train_correct / num_train_examples \n",
    "        train_loss = train_loss/ len(train_dl.dataset)  # len(train_dl.dataset) --> number of data integer\n",
    "\n",
    "        # ---EVALUATE ON VALIDATION SET ---\n",
    "        model.eval()\n",
    "        val_loss         = 0.0\n",
    "        num_val_correct  = 0\n",
    "        num_val_examples = 0\n",
    "\n",
    "        for batch in val_dl:\n",
    "            x = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "            yhat = model(x)\n",
    "            loss = loss_fn(yhat, y)\n",
    "\n",
    "            val_loss           += loss.data.item() * x.size(0)\n",
    "            num_val_correct    += (torch.max(yhat,1)[1] == y).sum().item()\n",
    "            num_val_examples   += y.shape[0]\n",
    "\n",
    "        val_acc = num_val_correct / num_val_examples\n",
    "        val_loss = val_loss/len(val_dl.dataset)\n",
    "\n",
    "        print('Epoch %3d/%3d, train loss: %5.2f, train acc: %5.2f, val loss: %5.2f, val acc: %5.2f' % \\\n",
    "                        (epoch, epochs, train_loss, train_acc, val_loss, val_acc))\n",
    "        #history['loss'].append(train_loss)\n",
    "        #history['val_loss'].append(val_loss)\n",
    "        #history['acc'].append(train_acc)\n",
    "        #history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # wandb collects all the info and provides interface\n",
    "        wandb.log({'epoch':epoch, 'train_loss':train_loss, 'val_loss':val_loss, 'train_acc':train_acc,'val_acc':val_acc})  \n",
    "        \n",
    "        #early stopping\n",
    "        #if val_loss <= val_loss_min:\n",
    "            #print(\"Validation loss Decreased {} --> {}. Saving model...\".format(val_loss_min, val_loss))\n",
    "            #torch.save(model.state_dict(),'models/ResNet1'+str(sweep_num)+'.pt')\n",
    "            #val_loss_min = val_loss\n",
    "            #torch.onnx.export(model, x, 'onnxmodels/ResNet1'+str(sweep_num)+\".onnx\")\n",
    "            #wandb.save('models/ResNet1'+str(sweep_num)+\".onnx\")\n",
    "    \n",
    "    # END OF TRAINING LOOP\n",
    "    end_time_sec       = time.time()\n",
    "    total_time_sec     = end_time_sec - start_time_sec\n",
    "    time_per_epoch_sec = total_time_sec / epochs\n",
    "    print()\n",
    "    print('Time total:     %5.2f sec' % (total_time_sec))\n",
    "    print('Time per epoch: %5.2f sec' % (time_per_epoch_sec))\n",
    "    \n",
    "    #wandb.finish()    \n",
    "    \n",
    "    #return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04f9df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(batch_size):\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71fe96a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y4p4sjiu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_planes: 43\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tk: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_blocks: [2, 2, 2]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\sudha\\Desktop\\my_files\\NYU sem 2\\Deep Learning\\Assignments\\ResNet_project_1\\2_ResNet_sweep\\wandb\\run-20220320_155916-y4p4sjiu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/runs/y4p4sjiu\" target=\"_blank\">brisk-sweep-1</a></strong> to <a href=\"https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/sweeps/ynexar88\" target=\"_blank\">https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/sweeps/ynexar88</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train() called: model=ResNet3, opt=Adamax(lr=0.010000), epochs=25, device=cuda\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>total_params</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_params</td><td>4991149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">brisk-sweep-1</strong>: <a href=\"https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/runs/y4p4sjiu\" target=\"_blank\">https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/runs/y4p4sjiu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220320_155916-y4p4sjiu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run y4p4sjiu errored: RuntimeError('mat1 dim 1 must match mat2 dim 0')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run y4p4sjiu errored: RuntimeError('mat1 dim 1 must match mat2 dim 0')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3nu7wscz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_planes: 43\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tk: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_blocks: [2, 2, 2]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\sudha\\Desktop\\my_files\\NYU sem 2\\Deep Learning\\Assignments\\ResNet_project_1\\2_ResNet_sweep\\wandb\\run-20220320_155926-3nu7wscz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/runs/3nu7wscz\" target=\"_blank\">jolly-sweep-2</a></strong> to <a href=\"https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/sweeps/ynexar88\" target=\"_blank\">https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/sweeps/ynexar88</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train() called: model=ResNet3, opt=Adamax(lr=0.050000), epochs=25, device=cuda\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>total_params</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_params</td><td>4991149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">jolly-sweep-2</strong>: <a href=\"https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/runs/3nu7wscz\" target=\"_blank\">https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/runs/3nu7wscz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220320_155926-3nu7wscz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 3nu7wscz errored: RuntimeError('mat1 dim 1 must match mat2 dim 0')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 3nu7wscz errored: RuntimeError('mat1 dim 1 must match mat2 dim 0')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ju2t15nd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_planes: 43\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tk: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_blocks: [2, 2, 2]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\sudha\\Desktop\\my_files\\NYU sem 2\\Deep Learning\\Assignments\\ResNet_project_1\\2_ResNet_sweep\\wandb\\run-20220320_155937-ju2t15nd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/runs/ju2t15nd\" target=\"_blank\">prime-sweep-3</a></strong> to <a href=\"https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/sweeps/ynexar88\" target=\"_blank\">https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/sweeps/ynexar88</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train() called: model=ResNet3, opt=Adamax(lr=0.010000), epochs=25, device=cuda\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>total_params</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_params</td><td>4991149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">prime-sweep-3</strong>: <a href=\"https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/runs/ju2t15nd\" target=\"_blank\">https://wandb.ai/deeplearning_project/ResNet-sweeps-local-sudu/runs/ju2t15nd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220320_155937-ju2t15nd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run ju2t15nd errored: RuntimeError('mat1 dim 1 must match mat2 dim 0')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ju2t15nd errored: RuntimeError('mat1 dim 1 must match mat2 dim 0')\n",
      "Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, sweep, count=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ba2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_gpu] *",
   "language": "python",
   "name": "conda-env-pytorch_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
