{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from time import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, kernel_size=3, dropout=0.2):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=kernel_size, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=kernel_size, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.dropout(out, self.dropout)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet3(nn.Module):\n",
    "    def __init__(self, block, num_blocks, in_planes, k=2, num_classes=10, kernel_size=3, dropout=0.2):\n",
    "        super(ResNet3, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.avg_pool_kernal_size = 4\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=kernel_size, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.layer1 = self._make_layer(block, k*self.in_planes, num_blocks[0], stride=1) # 32\n",
    "        self.layer2 = self._make_layer(block, k*self.in_planes, num_blocks[1], stride=2) # 64\n",
    "        self.layer3 = self._make_layer(block, k*self.in_planes, num_blocks[2], stride=2) #128\n",
    "        #self.layer4 = self._make_layer(block, k*self.in_planes, num_blocks[3], stride=2) #256\n",
    "        self.linear = nn.Linear(4*self.in_planes, num_classes)  #512 dense layers\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, self.kernel_size, self.dropout))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        #out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, self.avg_pool_kernal_size)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.dropout(out,self.dropout)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 300\n",
    "num_workers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeeplearning_project\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/sa6788/5_resnet_code/wandb/run-20220323_182125-2d5vpo0r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/deeplearning_project/teammatecode%20Resnet3%20inplane22%20numblock%287%2C7%2C7%29/runs/2d5vpo0r\" target=\"_blank\">eager-wood-2</a></strong> to <a href=\"https://wandb.ai/deeplearning_project/teammatecode%20Resnet3%20inplane22%20numblock%287%2C7%2C7%29\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "        project=\"teammatecode Resnet3 inplane22 numblock(7,7,7)\",\n",
    "        config={\n",
    "            \"epochs\": 300,           # Trainig epochs\n",
    "            \"optimizer\":\"sgd\",\n",
    "            \"scheduler\":'CosineAnnealingLR',\n",
    "            \"t_max\":100,\n",
    "            \"batch_size\": 128,       # batch size\n",
    "            \"lr\": 1e-2,              # Learning rate\n",
    "            \"in_planes\":22,          # no of channels in first conv layer\n",
    "            \"num_blocks\":(7,7,7), # num of ResNet block in each Residual layer\n",
    "            \"k\":2,                   # widening factor\n",
    "            \"classes\":10,\n",
    "            \"dataset\":\"CIFAR10\",\n",
    "            \"architecture\":\"Deep ResNet3\"\n",
    "            })\n",
    "\n",
    "# in_planes, num_blocks, k=2,\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### define transform\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get training and test sets\n",
    "train_data = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data loaders\n",
    "def loader(batch_size):\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, epoch, loss_fn, optimizer, model):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    \n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    train_loss_current = 0\n",
    "    train_current_corrects = 0\n",
    "    train_current_total = 0\n",
    "\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_current += loss.item()\n",
    "        _, predicted_class = y_pred.max(1)\n",
    "        train_current_total += y.size(0)\n",
    "        train_current_corrects += (predicted_class == y).sum().item()\n",
    "    \n",
    "    # Save Checkpoint\n",
    "    train_loss = train_loss_current/len(train_loader)\n",
    "    train_accuracy = 100*float(train_current_corrects) / train_current_total\n",
    "    \n",
    "    return train_loss, train_accuracy \n",
    "\n",
    "def test_model(test_loader, epoch, loss_fn, model):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    global best_acc\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    test_loss_current = 0\n",
    "    test_current_corrects = 0\n",
    "    test_current_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(test_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "    \n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "            test_loss_current += loss.item()\n",
    "\n",
    "            _, predicted_class = y_pred.max(1)\n",
    "            test_current_total += y.size(0)\n",
    "            test_current_corrects += (predicted_class == y).sum().item()\n",
    "    \n",
    "    # Save Checkpoint\n",
    "    test_loss = test_loss_current/len(test_loader)\n",
    "    test_accuracy = 100*float(test_current_corrects) / test_current_total\n",
    "    \n",
    "    if test_accuracy >= best_acc:\n",
    "            print(\"Accuracy increased {} --> {}. Saving model...\".format(best_acc, test_accuracy))\n",
    "            torch.save(model.state_dict(),'model.pt')\n",
    "            best_acc = test_accuracy\n",
    "            torch.onnx.export(model, X, \"model.onnx\")\n",
    "            wandb.save(\"model.onnx\")\n",
    "            \n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, scheduler, loss_fn, train_loader, test_loader):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    wandb.watch(model, loss_fn, log='all', log_freq=5000, log_graph=True)\n",
    "    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n",
    "              (type(model).__name__, type(optimizer).__name__,\n",
    "               optimizer.param_groups[0]['lr'], epochs, device))\n",
    "    \n",
    "    history             = {}\n",
    "    history['loss']     = []\n",
    "    history['val_loss'] = []\n",
    "    history['acc']      = []\n",
    "    history['val_acc']  = []\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_accuracy = train_model(train_loader, epoch, criterion, optimizer, model)\n",
    "        test_loss, test_accuracy = test_model(test_loader, epoch, criterion, model)\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(test_loss)\n",
    "        history['acc'].append(train_accuracy)\n",
    "        history['val_acc'].append(test_accuracy)\n",
    "        \n",
    "        wandb.log({'epoch':epoch, 'train_loss':train_loss, 'test_loss':test_loss, 'train_acc':train_accuracy,'test_accuracy':test_accuracy})  \n",
    "    \n",
    "    wandb.finish()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4976960\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "model = ResNet3(BasicBlock, (7,7,7),22, k = 2)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa6788/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 14, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_dl, test_dl = loader(128)\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.008, momentum = 0.9, weight_decay = 5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train() called: model=ResNet3, opt=SGD(lr=0.008000), epochs=300, device=cuda\n",
      "\n",
      "\n",
      "Epoch: 0\n",
      "Accuracy increased 0 --> 42.28. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa6788/.local/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py:773: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to training  mode. The operators will be exported in training , as specified by the functional operator.\n",
      "  warnings.warn(\"ONNX export mode is set to \" + training_mode +\n",
      "/home/sa6788/.local/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:1672: UserWarning: Dropout is a training op and should not be exported in inference mode. For inference, make sure to call eval() on the model and to export it with param training=False.\n",
      "  warnings.warn(\"Dropout is a training op and should not be exported in inference mode. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "Accuracy increased 42.28 --> 62.7. Saving model...\n",
      "\n",
      "Epoch: 2\n",
      "Accuracy increased 62.7 --> 66.44. Saving model...\n",
      "\n",
      "Epoch: 3\n",
      "Accuracy increased 66.44 --> 70.14. Saving model...\n",
      "\n",
      "Epoch: 4\n",
      "Accuracy increased 70.14 --> 73.5. Saving model...\n",
      "\n",
      "Epoch: 5\n",
      "Accuracy increased 73.5 --> 77.07. Saving model...\n",
      "\n",
      "Epoch: 6\n",
      "Accuracy increased 77.07 --> 79.05. Saving model...\n",
      "\n",
      "Epoch: 7\n",
      "Accuracy increased 79.05 --> 79.25. Saving model...\n",
      "\n",
      "Epoch: 8\n",
      "Accuracy increased 79.25 --> 80.57. Saving model...\n",
      "\n",
      "Epoch: 9\n",
      "Accuracy increased 80.57 --> 82.02. Saving model...\n",
      "\n",
      "Epoch: 10\n",
      "\n",
      "Epoch: 11\n",
      "Accuracy increased 82.02 --> 82.52. Saving model...\n",
      "\n",
      "Epoch: 12\n",
      "Accuracy increased 82.52 --> 83.81. Saving model...\n",
      "\n",
      "Epoch: 13\n",
      "Accuracy increased 83.81 --> 85.5. Saving model...\n",
      "\n",
      "Epoch: 14\n",
      "\n",
      "Epoch: 15\n",
      "Accuracy increased 85.5 --> 86.13. Saving model...\n",
      "\n",
      "Epoch: 16\n",
      "\n",
      "Epoch: 17\n",
      "\n",
      "Epoch: 18\n",
      "\n",
      "Epoch: 19\n",
      "Accuracy increased 86.13 --> 86.94. Saving model...\n",
      "\n",
      "Epoch: 20\n",
      "Accuracy increased 86.94 --> 87.35. Saving model...\n",
      "\n",
      "Epoch: 21\n",
      "Accuracy increased 87.35 --> 87.47. Saving model...\n",
      "\n",
      "Epoch: 22\n",
      "Accuracy increased 87.47 --> 87.9. Saving model...\n",
      "\n",
      "Epoch: 23\n",
      "\n",
      "Epoch: 24\n",
      "\n",
      "Epoch: 25\n",
      "\n",
      "Epoch: 26\n",
      "Accuracy increased 87.9 --> 89.1. Saving model...\n",
      "\n",
      "Epoch: 27\n",
      "\n",
      "Epoch: 28\n",
      "\n",
      "Epoch: 29\n",
      "Accuracy increased 89.1 --> 89.31. Saving model...\n",
      "\n",
      "Epoch: 30\n",
      "Accuracy increased 89.31 --> 89.33. Saving model...\n",
      "\n",
      "Epoch: 31\n",
      "\n",
      "Epoch: 32\n",
      "\n",
      "Epoch: 33\n",
      "\n",
      "Epoch: 34\n",
      "\n",
      "Epoch: 35\n",
      "Accuracy increased 89.33 --> 89.66. Saving model...\n",
      "\n",
      "Epoch: 36\n",
      "\n",
      "Epoch: 37\n",
      "Accuracy increased 89.66 --> 89.75. Saving model...\n",
      "\n",
      "Epoch: 38\n",
      "Accuracy increased 89.75 --> 89.9. Saving model...\n",
      "\n",
      "Epoch: 39\n",
      "Accuracy increased 89.9 --> 90.04. Saving model...\n",
      "\n",
      "Epoch: 40\n",
      "Accuracy increased 90.04 --> 90.15. Saving model...\n",
      "\n",
      "Epoch: 41\n",
      "Accuracy increased 90.15 --> 90.23. Saving model...\n",
      "\n",
      "Epoch: 42\n",
      "\n",
      "Epoch: 43\n",
      "\n",
      "Epoch: 44\n",
      "Accuracy increased 90.23 --> 90.37. Saving model...\n",
      "\n",
      "Epoch: 45\n",
      "Accuracy increased 90.37 --> 90.53. Saving model...\n",
      "\n",
      "Epoch: 46\n",
      "\n",
      "Epoch: 47\n",
      "Accuracy increased 90.53 --> 90.53. Saving model...\n",
      "\n",
      "Epoch: 48\n",
      "Accuracy increased 90.53 --> 90.54. Saving model...\n",
      "\n",
      "Epoch: 49\n",
      "Accuracy increased 90.54 --> 90.7. Saving model...\n",
      "\n",
      "Epoch: 50\n",
      "\n",
      "Epoch: 51\n",
      "Accuracy increased 90.7 --> 90.99. Saving model...\n",
      "\n",
      "Epoch: 52\n",
      "\n",
      "Epoch: 53\n",
      "\n",
      "Epoch: 54\n",
      "\n",
      "Epoch: 55\n",
      "Accuracy increased 90.99 --> 91.51. Saving model...\n",
      "\n",
      "Epoch: 56\n",
      "\n",
      "Epoch: 57\n",
      "\n",
      "Epoch: 58\n",
      "\n",
      "Epoch: 59\n",
      "\n",
      "Epoch: 60\n",
      "\n",
      "Epoch: 61\n",
      "\n",
      "Epoch: 62\n",
      "\n",
      "Epoch: 63\n",
      "Accuracy increased 91.51 --> 91.7. Saving model...\n",
      "\n",
      "Epoch: 64\n",
      "\n",
      "Epoch: 65\n",
      "\n",
      "Epoch: 66\n",
      "\n",
      "Epoch: 67\n",
      "Accuracy increased 91.7 --> 92.03. Saving model...\n",
      "\n",
      "Epoch: 68\n",
      "\n",
      "Epoch: 69\n",
      "\n",
      "Epoch: 70\n",
      "\n",
      "Epoch: 71\n",
      "Accuracy increased 92.03 --> 92.12. Saving model...\n",
      "\n",
      "Epoch: 72\n",
      "Accuracy increased 92.12 --> 92.19. Saving model...\n",
      "\n",
      "Epoch: 73\n",
      "Accuracy increased 92.19 --> 92.28. Saving model...\n",
      "\n",
      "Epoch: 74\n",
      "\n",
      "Epoch: 75\n",
      "Accuracy increased 92.28 --> 92.4. Saving model...\n",
      "\n",
      "Epoch: 76\n",
      "Accuracy increased 92.4 --> 92.53. Saving model...\n",
      "\n",
      "Epoch: 77\n",
      "\n",
      "Epoch: 78\n",
      "\n",
      "Epoch: 79\n",
      "Accuracy increased 92.53 --> 92.61. Saving model...\n",
      "\n",
      "Epoch: 80\n",
      "\n",
      "Epoch: 81\n",
      "Accuracy increased 92.61 --> 92.97. Saving model...\n",
      "\n",
      "Epoch: 82\n",
      "\n",
      "Epoch: 83\n",
      "\n",
      "Epoch: 84\n",
      "\n",
      "Epoch: 85\n",
      "\n",
      "Epoch: 86\n",
      "\n",
      "Epoch: 87\n",
      "\n",
      "Epoch: 88\n",
      "\n",
      "Epoch: 89\n",
      "\n",
      "Epoch: 90\n",
      "\n",
      "Epoch: 91\n",
      "Accuracy increased 92.97 --> 92.99. Saving model...\n",
      "\n",
      "Epoch: 92\n",
      "\n",
      "Epoch: 93\n",
      "\n",
      "Epoch: 94\n",
      "Accuracy increased 92.99 --> 93.14. Saving model...\n",
      "\n",
      "Epoch: 95\n",
      "\n",
      "Epoch: 96\n",
      "\n",
      "Epoch: 97\n",
      "\n",
      "Epoch: 98\n",
      "\n",
      "Epoch: 99\n",
      "\n",
      "Epoch: 100\n",
      "\n",
      "Epoch: 101\n",
      "\n",
      "Epoch: 102\n",
      "\n",
      "Epoch: 103\n",
      "\n",
      "Epoch: 104\n",
      "\n",
      "Epoch: 105\n",
      "\n",
      "Epoch: 106\n",
      "\n",
      "Epoch: 107\n",
      "Accuracy increased 93.14 --> 93.14. Saving model...\n",
      "\n",
      "Epoch: 108\n",
      "\n",
      "Epoch: 109\n",
      "\n",
      "Epoch: 110\n",
      "\n",
      "Epoch: 111\n",
      "\n",
      "Epoch: 112\n",
      "\n",
      "Epoch: 113\n",
      "\n",
      "Epoch: 114\n",
      "\n",
      "Epoch: 115\n",
      "\n",
      "Epoch: 116\n",
      "\n",
      "Epoch: 117\n",
      "\n",
      "Epoch: 118\n",
      "\n",
      "Epoch: 119\n",
      "\n",
      "Epoch: 120\n",
      "\n",
      "Epoch: 121\n",
      "\n",
      "Epoch: 122\n",
      "\n",
      "Epoch: 123\n",
      "\n",
      "Epoch: 124\n",
      "\n",
      "Epoch: 125\n",
      "\n",
      "Epoch: 126\n",
      "\n",
      "Epoch: 127\n",
      "\n",
      "Epoch: 128\n",
      "\n",
      "Epoch: 129\n",
      "\n",
      "Epoch: 130\n",
      "\n",
      "Epoch: 131\n",
      "\n",
      "Epoch: 132\n",
      "\n",
      "Epoch: 133\n",
      "\n",
      "Epoch: 134\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "train(model, config.epochs,optimizer,scheduler,criterion,train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
